# AIRD ML ë°©ë²•ë¡  ê°€ì´ë“œ

## ğŸ“‹ ë¬¸ì„œ ì •ë³´
- **ë²„ì „**: 1.0
- **ì‘ì„±ì¼**: 2025-11-28
- **ëŒ€ìƒ**: ë°ì´í„° ê³¼í•™ì, ML ì—”ì§€ë‹ˆì–´, ë¶„ì„ê°€

---

## ğŸ¯ ê°œìš”

ì´ ë¬¸ì„œëŠ” AIRD ML Packì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ë¡ ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.  
ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í•™ìŠµ, í•´ì„, í‰ê°€ê¹Œì§€ ì „ ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ“Š ì „ì²´ ML íŒŒì´í”„ë¼ì¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIRD ML Pipeline                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ë°ì´í„° ì „ì²˜ë¦¬
   â”œâ”€ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
   â”œâ”€ ì´ìƒì¹˜ ì œê±°
   â”œâ”€ Feature Engineering
   â””â”€ ìŠ¤ì¼€ì¼ë§

          â†“

2. Feature ì„ íƒ
   â”œâ”€ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜
   â”œâ”€ ìƒê´€ê´€ê³„ ë¶„ì„
   â””â”€ Feature Importance

          â†“

3. ë°ì´í„° ë¶„í• 
   â”œâ”€ Train: 70%
   â”œâ”€ Validation: 15%
   â””â”€ Test: 15%

          â†“

4. ëª¨ë¸ ì„ íƒ & í•™ìŠµ
   â”œâ”€ Baseline: Logistic Regression
   â”œâ”€ Advanced: Random Forest
   â””â”€ Expert: XGBoost

          â†“

5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
   â”œâ”€ Grid Search
   â”œâ”€ Random Search
   â””â”€ Bayesian Optimization

          â†“

6. ëª¨ë¸ í‰ê°€
   â”œâ”€ Accuracy, Precision, Recall
   â”œâ”€ F1-Score, AUC-ROC
   â””â”€ Confusion Matrix

          â†“

7. ëª¨ë¸ í•´ì„
   â”œâ”€ Feature Importance
   â”œâ”€ SHAP Values
   â””â”€ Partial Dependence

          â†“

8. ë°°í¬ & ëª¨ë‹ˆí„°ë§
   â”œâ”€ ëª¨ë¸ ì €ì¥
   â”œâ”€ ì˜ˆì¸¡ API
   â””â”€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```

---

## 1ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬

### 1.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬

#### ì „ëµë³„ ì ìš© ì‹œì 

| ì „ëµ | ì ìš© ì¡°ê±´ | ì˜ˆì‹œ |
|------|----------|------|
| ì œê±° | ê²°ì¸¡ < 5% | ì „í™”ë²ˆí˜¸ |
| ì¤‘ìœ„ê°’ ëŒ€ì²´ | ìˆ˜ì¹˜í˜•, ê²°ì¸¡ 5-20% | ì œì¡°ì‹œì„¤ë©´ì  |
| ìµœë¹ˆê°’ ëŒ€ì²´ | ë²”ì£¼í˜•, ê²°ì¸¡ 5-20% | ê³µì¥ê·œëª¨ |
| ëª¨ë¸ ê¸°ë°˜ ëŒ€ì²´ | ì¤‘ìš” ë³€ìˆ˜, ê²°ì¸¡ > 20% | ì¢…ì—…ì›ìˆ˜ |

#### êµ¬í˜„ ì˜ˆì‹œ

```python
from sklearn.impute import SimpleImputer, KNNImputer

# ìˆ˜ì¹˜í˜•: ì¤‘ìœ„ê°’
num_imputer = SimpleImputer(strategy='median')
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

# ë²”ì£¼í˜•: ìµœë¹ˆê°’
cat_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

# ê³ ê¸‰: KNN Imputer
knn_imputer = KNNImputer(n_neighbors=5)
df[important_cols] = knn_imputer.fit_transform(df[important_cols])
```

---

### 1.2 ì´ìƒì¹˜ ì²˜ë¦¬

#### IQR ë°©ë²• (Interquartile Range)

```python
def remove_outliers_iqr(df, column, threshold=1.5):
    """
    IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°
    
    Parameters:
    - threshold: 1.5 (ë³´í†µ), 3.0 (ê·¹ë‹¨ê°’ë§Œ)
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - threshold * IQR
    upper_bound = Q3 + threshold * IQR
    
    # ì´ìƒì¹˜ë¥¼ NaNìœ¼ë¡œ ì²˜ë¦¬
    df.loc[(df[column] < lower_bound) | 
           (df[column] > upper_bound), column] = np.nan
    
    return df

# ì ìš©
df = remove_outliers_iqr(df, 'feature_base_area', threshold=3)
```

#### Z-Score ë°©ë²•

```python
from scipy import stats

def remove_outliers_zscore(df, column, threshold=3):
    """Z-Score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°"""
    z_scores = np.abs(stats.zscore(df[column].dropna()))
    df.loc[z_scores > threshold, column] = np.nan
    return df
```

---

### 1.3 Feature Engineering

#### 1.3.1 ê³µì¥ ì—°ë ¹ ê³„ì‚°

```python
def calculate_factory_age(df, date_col='ìµœì´ˆìŠ¹ì¸ì¼', base_year=2025):
    """
    ê³µì¥ ì—°ë ¹ ê³„ì‚°
    
    Returns:
    - ì—°ë ¹ (ë…„ ë‹¨ìœ„)
    """
    dates = pd.to_datetime(df[date_col], format='%Y%m%d', errors='coerce')
    df['ê³µì¥ì—°ë ¹'] = base_year - dates.dt.year
    
    # ìŒìˆ˜ ë°©ì§€
    df['ê³µì¥ì—°ë ¹'] = df['ê³µì¥ì—°ë ¹'].clip(lower=0)
    
    return df
```

#### 1.3.2 ë¡œê·¸ ë³€í™˜

```python
def log_transform(df, columns):
    """
    ì™œë„ê°€ í° ë³€ìˆ˜ì— ë¡œê·¸ ë³€í™˜ ì ìš©
    
    ì ìš© ê¸°ì¤€:
    - ì™œë„(skewness) > 1
    - ëª¨ë“  ê°’ > 0
    """
    for col in columns:
        if df[col].min() > 0 and df[col].skew() > 1:
            df[f'{col}_log'] = np.log1p(df[col])
    
    return df

# ì ìš©
df = log_transform(df, ['ì œì¡°ì‹œì„¤ë©´ì ', 'ìš©ì§€ë©´ì '])
```

#### 1.3.3 ì§‘ê³„ Feature

```python
def create_aggregation_features(df, group_col, agg_col):
    """
    ê·¸ë£¹ë³„ ì§‘ê³„ Feature ìƒì„±
    
    ì˜ˆì‹œ: ë™ë³„ ê³µì¥ ìˆ˜, í‰ê·  ë©´ì 
    """
    # ì¹´ìš´íŠ¸
    count_feature = df.groupby(group_col)[agg_col].count()
    df[f'{group_col}_factory_count'] = df[group_col].map(count_feature)
    
    # í‰ê· 
    mean_feature = df.groupby(group_col)[agg_col].mean()
    df[f'{group_col}_avg_{agg_col}'] = df[group_col].map(mean_feature)
    
    return df

# ì ìš©
df = create_aggregation_features(df, 'ê³µì¥ë™', 'ì œì¡°ì‹œì„¤ë©´ì ')
```

#### 1.3.4 ë°€ì§‘ë„ Feature

```python
from sklearn.preprocessing import MinMaxScaler

def create_density_feature(df, region_col='ì‹œêµ°êµ¬ëª…'):
    """
    ì§€ì—­ë³„ ë°€ì§‘ë„ ì ìˆ˜ (0-1)
    """
    # ì§€ì—­ë³„ ê³µì¥ ìˆ˜
    density = df[region_col].value_counts() / len(df)
    df['ìì¹˜êµ¬_ë°€ì§‘ë„'] = df[region_col].map(density)
    
    # MinMax ìŠ¤ì¼€ì¼ë§
    scaler = MinMaxScaler()
    df['ìì¹˜êµ¬_ë°€ì§‘ë„ì ìˆ˜'] = scaler.fit_transform(
        df[['ìì¹˜êµ¬_ë°€ì§‘ë„']]
    )
    
    return df
```

---

### 1.4 ìŠ¤ì¼€ì¼ë§

#### StandardScaler (í‘œì¤€í™”)

```python
from sklearn.preprocessing import StandardScaler

# í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# ì ìš© ì‹œì : Tree ê¸°ë°˜ ëª¨ë¸ì—ëŠ” ë¶ˆí•„ìš”
# ì‚¬ìš© ëª¨ë¸: Logistic Regression, SVM, Neural Networks
```

#### MinMaxScaler (ì •ê·œí™”)

```python
from sklearn.preprocessing import MinMaxScaler

# 0-1 ë²”ìœ„ë¡œ ë³€í™˜
scaler = MinMaxScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# ì ìš© ì‹œì : ë²”ìœ„ê°€ ì¤‘ìš”í•œ ê²½ìš°
# ì‚¬ìš© ì˜ˆ: ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚°
```

---

## 2ï¸âƒ£ Feature ì„ íƒ

### 2.1 ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜

**ê³µì¥ ë¦¬ìŠ¤í¬ ì˜ˆì¸¡ì„ ìœ„í•œ í•µì‹¬ Feature**:
1. `feature_base_age`: ë…¸í›„ë„
2. `feature_base_area_log`: ê·œëª¨
3. `feature_base_gu_density`: ë°€ì§‘ë„
4. `feature_agg_dong_factory_count`: ì£¼ë³€ í™˜ê²½

**ì„ íƒ ê¸°ì¤€**:
- ì •ì±…ì  ì¤‘ìš”ì„±
- ë°ì´í„° ê°€ìš©ì„±
- í•´ì„ ê°€ëŠ¥ì„±

---

### 2.2 ìƒê´€ê´€ê³„ ë¶„ì„

```python
import seaborn as sns
import matplotlib.pyplot as plt

# ìƒê´€ê´€ê³„ í–‰ë ¬
corr_matrix = df[feature_cols].corr()

# íˆíŠ¸ë§µ
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature ìƒê´€ê´€ê³„')
plt.tight_layout()
plt.show()

# ë‹¤ì¤‘ê³µì„ ì„± ì œê±° (ìƒê´€ê³„ìˆ˜ > 0.9)
high_corr = (corr_matrix.abs() > 0.9) & (corr_matrix != 1.0)
to_drop = [col for col in high_corr.columns if high_corr[col].any()]
```

---

### 2.3 Feature Importance

```python
from sklearn.ensemble import RandomForestClassifier

# ëª¨ë¸ í•™ìŠµ
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Feature Importance
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.barh(importance_df['feature'], importance_df['importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.tight_layout()
plt.show()

# ì¤‘ìš”ë„ ë‚®ì€ Feature ì œê±° (< 0.01)
important_features = importance_df[
    importance_df['importance'] >= 0.01
]['feature'].tolist()
```

---

## 3ï¸âƒ£ ë°ì´í„° ë¶„í• 

### 3.1 Train-Validation-Test Split

```python
from sklearn.model_selection import train_test_split

# Step 1: Train + Val (85%) vs Test (15%)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)

# Step 2: Train (70%) vs Val (15%)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp
)
# 0.176 = 15 / 85 (ì „ì²´ì˜ 15%ê°€ ë˜ë„ë¡)

print(f"Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Val: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)")
print(f"Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
```

### 3.2 Stratified Split (ë¶ˆê· í˜• ë°ì´í„°)

```python
# í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
train_test_split(X, y, stratify=y, test_size=0.2)

# í™•ì¸
print("Train class distribution:")
print(y_train.value_counts(normalize=True))
print("\nTest class distribution:")
print(y_test.value_counts(normalize=True))
```

---

## 4ï¸âƒ£ ëª¨ë¸ ì„ íƒ & í•™ìŠµ

### 4.1 Baseline ëª¨ë¸: Logistic Regression

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# ëª¨ë¸ ì •ì˜
lr = LogisticRegression(
    max_iter=1000,
    random_state=42,
    class_weight='balanced'  # ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘
)

# í•™ìŠµ
lr.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = lr.predict(X_test)
y_pred_proba = lr.predict_proba(X_test)[:, 1]

# í‰ê°€
print("Logistic Regression Results:")
print(classification_report(y_test, y_pred))
print(f"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.3f}")
```

**ì¥ì **:
- ë¹ ë¥¸ í•™ìŠµ
- í•´ì„ ìš©ì´
- ì•ˆì •ì 

**ë‹¨ì **:
- ì„ í˜• ê´€ê³„ë§Œ í¬ì°©
- ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ì–´ë ¤ì›€

---

### 4.2 Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

# ëª¨ë¸ ì •ì˜
rf = RandomForestClassifier(
    n_estimators=100,      # íŠ¸ë¦¬ ê°œìˆ˜
    max_depth=10,          # ìµœëŒ€ ê¹Šì´
    min_samples_split=20,  # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
    min_samples_leaf=10,   # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ
    random_state=42,
    n_jobs=-1,             # ë³‘ë ¬ ì²˜ë¦¬
    class_weight='balanced'
)

# í•™ìŠµ
rf.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = rf.predict(X_test)
y_pred_proba = rf.predict_proba(X_test)[:, 1]

# í‰ê°€
print("Random Forest Results:")
print(classification_report(y_test, y_pred))
print(f"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.3f}")
```

**ì¥ì **:
- ë†’ì€ ì •í™•ë„
- ê³¼ì í•© ë°©ì§€
- Feature Importance ì œê³µ

**ë‹¨ì **:
- í•™ìŠµ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ

---

### 4.3 XGBoost

```python
from xgboost import XGBClassifier

# ëª¨ë¸ ì •ì˜
xgb = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='auc',
    early_stopping_rounds=10
)

# í•™ìŠµ (Validation Set ì‚¬ìš©)
xgb.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=False
)

# ì˜ˆì¸¡
y_pred = xgb.predict(X_test)
y_pred_proba = xgb.predict_proba(X_test)[:, 1]

# í‰ê°€
print("XGBoost Results:")
print(classification_report(y_test, y_pred))
print(f"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.3f}")
```

**ì¥ì **:
- ìµœê³  ìˆ˜ì¤€ ì„±ëŠ¥
- ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬
- ë¹ ë¥¸ í•™ìŠµ (GPU ì§€ì›)

**ë‹¨ì **:
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë§ìŒ
- í•´ì„ ì–´ë ¤ì›€ (SHAP í•„ìš”)

---

## 5ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### 5.1 Grid Search

```python
from sklearn.model_selection import GridSearchCV

# íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [10, 20, 30]
}

# Grid Search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

# ì‹¤í–‰
grid_search.fit(X_train, y_train)

# ìµœì  íŒŒë¼ë¯¸í„°
print("Best parameters:", grid_search.best_params_)
print("Best AUC:", grid_search.best_score_)

# ìµœì  ëª¨ë¸
best_model = grid_search.best_estimator_
```

---

### 5.2 Random Search

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# íŒŒë¼ë¯¸í„° ë¶„í¬
param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(5, 20),
    'min_samples_split': randint(10, 50),
    'min_samples_leaf': randint(5, 20)
}

# Random Search
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_dist,
    n_iter=20,  # ì‹œë„ íšŸìˆ˜
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42
)

# ì‹¤í–‰
random_search.fit(X_train, y_train)

print("Best parameters:", random_search.best_params_)
```

---

## 6ï¸âƒ£ ëª¨ë¸ í‰ê°€

### 6.1 ë¶„ë¥˜ ì§€í‘œ

#### Confusion Matrix

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# ì‹œê°í™”
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# í•´ì„
TN, FP, FN, TP = cm.ravel()
print(f"True Negative: {TN}")
print(f"False Positive: {FP}")
print(f"False Negative: {FN}")
print(f"True Positive: {TP}")
```

#### ì •ë°€ë„, ì¬í˜„ìœ¨, F1-Score

```python
from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-Score: {f1:.3f}")
```

**í•´ì„**:
- **Precision (ì •ë°€ë„)**: ê³ ìœ„í—˜ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ê³ ìœ„í—˜ ë¹„ìœ¨
- **Recall (ì¬í˜„ìœ¨)**: ì‹¤ì œ ê³ ìœ„í—˜ ì¤‘ ê³ ìœ„í—˜ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
- **F1-Score**: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· 

---

### 6.2 ROC Curve & AUC

```python
from sklearn.metrics import roc_curve, roc_auc_score

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)

# ì‹œê°í™”
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

**í•´ì„**:
- **AUC = 1.0**: ì™„ë²½í•œ ë¶„ë¥˜
- **AUC = 0.9-1.0**: ìš°ìˆ˜
- **AUC = 0.8-0.9**: ì–‘í˜¸
- **AUC = 0.7-0.8**: ë³´í†µ
- **AUC < 0.7**: ê°œì„  í•„ìš”

---

## 7ï¸âƒ£ ëª¨ë¸ í•´ì„

### 7.1 Feature Importance

```python
import matplotlib.pyplot as plt

# Feature Importance ì¶”ì¶œ
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices])
plt.xticks(range(len(importances)), 
           [feature_cols[i] for i in indices], 
           rotation=45, ha='right')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.tight_layout()
plt.show()

# ìƒìœ„ 10ê°œ Feature
for i in range(10):
    idx = indices[i]
    print(f"{i+1}. {feature_cols[idx]}: {importances[idx]:.4f}")
```

---

### 7.2 SHAP (SHapley Additive exPlanations)

```python
import shap

# SHAP Explainer ìƒì„±
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test)

# Summary Plot
shap.summary_plot(shap_values[1], X_test, feature_names=feature_cols)

# Force Plot (ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…)
shap.force_plot(
    explainer.expected_value[1],
    shap_values[1][0],
    X_test.iloc[0],
    feature_names=feature_cols
)

# Dependence Plot (Featureì™€ Target ê´€ê³„)
shap.dependence_plot(
    'feature_base_age',
    shap_values[1],
    X_test,
    feature_names=feature_cols
)
```

**í•´ì„**:
- **ë¹¨ê°„ìƒ‰**: ê³ ìœ„í—˜ ë°©í–¥ìœ¼ë¡œ ì˜í–¥
- **íŒŒë€ìƒ‰**: ì €ìœ„í—˜ ë°©í–¥ìœ¼ë¡œ ì˜í–¥
- **í¬ê¸°**: ì˜í–¥ë ¥ì˜ í¬ê¸°

---

### 7.3 Partial Dependence Plot

```python
from sklearn.inspection import PartialDependenceDisplay

# Partial Dependence Plot
features = ['feature_base_age', 'feature_base_area_log']
PartialDependenceDisplay.from_estimator(
    rf, X_train, features, 
    grid_resolution=50
)
plt.tight_layout()
plt.show()
```

---

## 8ï¸âƒ£ ëª¨ë¸ ì €ì¥ & ë°°í¬

### 8.1 ëª¨ë¸ ì €ì¥

```python
import joblib

# ëª¨ë¸ ì €ì¥
joblib.dump(rf, 'factory_risk_model_v1.pkl')

# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
joblib.dump(scaler, 'scaler_v1.pkl')

# Feature ë¦¬ìŠ¤íŠ¸ ì €ì¥
with open('features_v1.txt', 'w') as f:
    f.write('\n'.join(feature_cols))

print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
```

### 8.2 ëª¨ë¸ ë¡œë“œ & ì˜ˆì¸¡

```python
# ëª¨ë¸ ë¡œë“œ
model = joblib.load('factory_risk_model_v1.pkl')
scaler = joblib.load('scaler_v1.pkl')

with open('features_v1.txt', 'r') as f:
    features = [line.strip() for line in f]

# ìƒˆ ë°ì´í„° ì˜ˆì¸¡
def predict_risk(new_data):
    """ìƒˆë¡œìš´ ê³µì¥ ë°ì´í„°ì˜ ë¦¬ìŠ¤í¬ ì˜ˆì¸¡"""
    # Feature ì¶”ì¶œ
    X_new = new_data[features]
    
    # ìŠ¤ì¼€ì¼ë§
    X_scaled = scaler.transform(X_new)
    
    # ì˜ˆì¸¡
    prediction = model.predict(X_scaled)
    probability = model.predict_proba(X_scaled)[:, 1]
    
    return {
        'risk_label': 'ê³ ìœ„í—˜' if prediction[0] == 1 else 'ì €ìœ„í—˜',
        'risk_probability': probability[0]
    }

# ì‚¬ìš© ì˜ˆì‹œ
result = predict_risk(new_factory_data)
print(f"ë¦¬ìŠ¤í¬: {result['risk_label']}")
print(f"í™•ë¥ : {result['risk_probability']:.2%}")
```

---

## 9ï¸âƒ£ ëª¨ë¸ ëª¨ë‹ˆí„°ë§

### 9.1 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
class ModelMonitor:
    """ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, model, threshold=0.05):
        self.model = model
        self.baseline_auc = None
        self.threshold = threshold  # í—ˆìš© ì„±ëŠ¥ ì €í•˜
    
    def set_baseline(self, X_test, y_test):
        """ê¸°ì¤€ì„  ì„¤ì •"""
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        self.baseline_auc = roc_auc_score(y_test, y_pred_proba)
        print(f"Baseline AUC: {self.baseline_auc:.3f}")
    
    def check_performance(self, X_new, y_new):
        """ì„±ëŠ¥ ì²´í¬"""
        y_pred_proba = self.model.predict_proba(X_new)[:, 1]
        current_auc = roc_auc_score(y_new, y_pred_proba)
        
        degradation = self.baseline_auc - current_auc
        
        if degradation > self.threshold:
            print(f"âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€!")
            print(f"Baseline: {self.baseline_auc:.3f}")
            print(f"Current: {current_auc:.3f}")
            print(f"Degradation: {degradation:.3f}")
            return False
        else:
            print(f"âœ… ì„±ëŠ¥ ì •ìƒ (AUC: {current_auc:.3f})")
            return True

# ì‚¬ìš©
monitor = ModelMonitor(rf)
monitor.set_baseline(X_test, y_test)
monitor.check_performance(X_new, y_new)
```

---

## ğŸ¯ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### ìƒí™©ë³„ ì¶”ì²œ ëª¨ë¸

| ìƒí™© | ì¶”ì²œ ëª¨ë¸ | ì´ìœ  |
|------|----------|------|
| ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… | Logistic Regression | ë¹ ë¥´ê³  ì•ˆì •ì  |
| ë†’ì€ ì •í™•ë„ í•„ìš” | Random Forest, XGBoost | ìµœê³  ì„±ëŠ¥ |
| í•´ì„ ì¤‘ìš” | Logistic Regression + SHAP | ì„¤ëª… ìš©ì´ |
| ëŒ€ìš©ëŸ‰ ë°ì´í„° | XGBoost | ë©”ëª¨ë¦¬ íš¨ìœ¨ì  |
| ë¶ˆê· í˜• ë°ì´í„° | class_weight='balanced' | í´ë˜ìŠ¤ ê· í˜• ì¡°ì • |

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ì±…
- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
- "Introduction to Statistical Learning" - James et al.

### ì˜¨ë¼ì¸ ì½”ìŠ¤
- Coursera: Machine Learning (Andrew Ng)
- Fast.ai: Practical Deep Learning

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œ
- [scikit-learn](https://scikit-learn.org/)
- [XGBoost](https://xgboost.readthedocs.io/)
- [SHAP](https://shap.readthedocs.io/)

---

**AIRD ML Methodology v1.0**  
**ì‘ì„±ì¼**: 2025-11-28  
**ë¬¸ì˜**: aird-support@example.com
